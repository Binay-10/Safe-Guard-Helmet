"""
Basic NIRBHAYA App Without Login or Cloud

Features:
1. Face detection (OpenCV)
2. Audio recording and playback (sounddevice)
3. GPS location detection (geocoder)
4. Emotion detection (Happy, Neutral, Sad, Nervous)
"""

import sys
import os
import cv2
import sounddevice as sd
from scipy.io.wavfile import write
import geocoder
import numpy as np
import mediapipe as mp

try:
    from PyQt6.QtWidgets import QApplication, QWidget, QPushButton, QVBoxLayout, QMessageBox
except ModuleNotFoundError:
    print("❌ PyQt6 is not installed. Please run: pip install PyQt6")
    sys.exit(1)

# ---- Feature 1: Face Detection ----
def detect_face():
    cam = cv2.VideoCapture(0)
    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")

    while True:
        ret, frame = cam.read()
        if not ret:
            break
        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
        faces = face_cascade.detectMultiScale(gray, 1.1, 4)
        for (x, y, w, h) in faces:
            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)
        cv2.imshow("Face Detection", frame)
        if cv2.waitKey(1) == ord('q'):
            break

    cam.release()
    cv2.destroyAllWindows()

# ---- Feature 2: Voice Recording ----
def record_voice(duration=5):
    fs = 44100
    print("🎙️ Recording...")
    audio = sd.rec(int(duration * fs), samplerate=fs, channels=2)
    sd.wait()
    write("voice_recording.wav", fs, audio)
    print("✅ Saved as voice_recording.wav")

def play_voice():
    print("🔊 Playing recording...")
    os.system("start voice_recording.wav")  # Windows only

# ---- Feature 3: GPS Location ----
def get_location():
    g = geocoder.ip('me')
    if g.ok:
        return g.latlng
    else:
        return None

# ---- Feature 4: Emotion Detection ----
def detect_emotion():
    mp_face_mesh = mp.solutions.face_mesh
    drawing = mp.solutions.drawing_utils

    def classify_emotion(landmarks):
        # Improved logic using multiple facial landmarks
        top_lip = landmarks[13].y
        bottom_lip = landmarks[14].y
        left_eyebrow = landmarks[65].y
        right_eyebrow = landmarks[295].y
        nose_tip = landmarks[1].y

        mouth_openness = bottom_lip - top_lip
        brow_drop = ((left_eyebrow + right_eyebrow) / 2) - nose_tip

        if mouth_openness > 0.045:
            return "Happy"
        elif mouth_openness > 0.02 and brow_drop > 0.02:
            return "Nervous"
        elif mouth_openness > 0.015:
            return "Neutral"
        else:
            return "Sad"

    cam = cv2.VideoCapture(0)
    with mp_face_mesh.FaceMesh(min_detection_confidence=0.5, min_tracking_confidence=0.5) as face_mesh:
        while True:
            ret, frame = cam.read()
            if not ret:
                break
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            result = face_mesh.process(rgb)

            if result.multi_face_landmarks:
                for face in result.multi_face_landmarks:
                    drawing.draw_landmarks(frame, face, mp_face_mesh.FACEMESH_TESSELATION)
                    emotion = classify_emotion(face.landmark)
                    cv2.putText(frame, f"Emotion: {emotion}", (30, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

            cv2.imshow("Emotion Detection", frame)
            if cv2.waitKey(1) & 0xFF == ord('q'):
                break

    cam.release()
    cv2.destroyAllWindows()

# ---- GUI ----
class NirbhayaApp(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("NIRBHAYA: The Securix")
        self.resize(300, 400)

        layout = QVBoxLayout()

        face_btn = QPushButton("Start Face Detection")
        emotion_btn = QPushButton("Detect Emotion")
        record_btn = QPushButton("Record Voice")
        play_btn = QPushButton("Play Recording")
        loc_btn = QPushButton("Show Location")

        face_btn.clicked.connect(detect_face)
        emotion_btn.clicked.connect(detect_emotion)
        record_btn.clicked.connect(lambda: record_voice(5))
        play_btn.clicked.connect(play_voice)
        loc_btn.clicked.connect(self.show_location)

        layout.addWidget(face_btn)
        layout.addWidget(emotion_btn)
        layout.addWidget(record_btn)
        layout.addWidget(play_btn)
        layout.addWidget(loc_btn)

        self.setLayout(layout)

    def show_location(self):
        loc = get_location()
        if loc:
            QMessageBox.information(self, "Location", f"Latitude: {loc[0]}\nLongitude: {loc[1]}")
        else:
            QMessageBox.warning(self, "Location", "Unable to get location.")

# ---- Entry Point ----
if __name__ == "__main__":
    app = QApplication(sys.argv)
    main_app = NirbhayaApp()
    main_app.show()
    sys.exit(app.exec())
